{% set name = "llama-index" %}
{% set version = "0.9.6post2" %}

package:
  name: {{ name|lower }}
  version: {{ version }}

source:
  url: https://pypi.io/packages/source/{{ name[0] }}/{{ name }}/llama_index-{{ version }}.tar.gz
  sha256: 14cca43188d9afd058d74c5574e745dc937e724a20576a9ef81967b9f2659b03

build:
  noarch: python
  script: {{ PYTHON }} -m pip install . -vv
  number: 0

requirements:
  host:
    - python >=3.8,<3.12
    - pip
  run:
    - python >=3.8,<=3.12
    - SQLAlchemy ={extras = ["asyncio"], version = ">=1.4.49"}
    - beautifulsoup4 ="^4.12.2"
    - dataclasses-json ="*"
    - deprecated =">=1.2.9.3"
    - fsspec =">=2023.5.0"
    - httpx ="*"
    - langchain ={optional = true, version = ">=0.0.303"}
    - nest-asyncio ="^1.5.8"
    - nltk ="^3.8.1"
    - numpy ="*"
    - openai =">=1.1.0"
    - pandas ="*"
    - tenacity =">=8.2.0,<9.0.0"
    - tiktoken =">=0.3.3"
    - typing-extensions =">=4.5.0"
    - typing-inspect =">=0.8.0"
    - requests =">=2.31.0"
    - asyncpg ={optional = true, version = "^0.28.0"}
    - pgvector ={optional = true, version = "^0.1.0"}
    - psycopg-binary ={optional = true, version = "^3.1.12"}
    - optimum ={extras = ["onnxruntime"], optional = true, version = "^1.13.2"}
    - sentencepiece ={optional = true, version = "^0.1.99"}
    - transformers ={extras = ["torch"], optional = true, version = "^4.34.0"}
    - guidance ={optional = true, version = "^0.0.64"}
    - lm-format-enforcer ={optional = true, version = "^0.4.3"}
    - jsonpath-ng ={optional = true, version = "^1.6.0"}
    - rank-bm25 ={optional = true, version = "^0.2.2"}
    - scikit-learn ={optional = true, version = "<1.3.0"}
    - spacy ={optional = true, version = "^3.7.1"}
    - aiostream ="^0.5.2"
    - aiohttp ="^3.8.6"

test:
  imports:
    - llama_index
  commands:
    - pip check
  requires:
    - pip

about:
  home: https://github.com/run-llama/llama_index
  summary: Interface between LLMs and your data
  license: MIT
  license_file: LICENSE

extra:
  recipe-maintainers:
    - pavelzw
